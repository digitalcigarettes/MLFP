{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508d7427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067609ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n",
      "[(['hi'], 'greeting'), (['hey'], 'greeting'), (['how', 'are', 'you'], 'greeting'), (['is', 'anyone', 'there'], 'greeting'), (['hello'], 'greeting'), (['good', 'day'], 'greeting'), (['bye'], 'goodbye'), (['see', 'you', 'later'], 'goodbye'), (['goodbye'], 'goodbye'), (['thanks'], 'thanks'), (['thank', 'you'], 'thanks'), (['that', 's', 'helpful'], 'thanks'), (['thank', 's', 'a', 'lot'], 'thanks'), (['which', 'items', 'do', 'you', 'have'], 'items'), (['what', 'kinds', 'of', 'items', 'are', 'there'], 'items'), (['what', 'do', 'you', 'sell'], 'items'), (['do', 'you', 'take', 'credit', 'cards'], 'payments'), (['do', 'you', 'accept', 'mastercard'], 'payments'), (['can', 'i', 'pay', 'with', 'paypal'], 'payments'), (['are', 'you', 'cash', 'only'], 'payments'), (['how', 'long', 'does', 'delivery', 'take'], 'delivery'), (['how', 'long', 'does', 'shipping', 'take'], 'delivery'), (['when', 'do', 'i', 'get', 'my', 'delivery'], 'delivery'), (['tell', 'me', 'a', 'joke'], 'funny'), (['tell', 'me', 'something', 'funny'], 'funny'), (['do', 'you', 'know', 'a', 'joke'], 'funny')]\n"
     ]
    }
   ],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "df = open('context_responses.json').read()\n",
    "intents = json.loads(df)\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        pattern = normalize(pattern)\n",
    "        word_list = pattern.split(\" \")\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list,intent['tag']))\n",
    "        \n",
    "        if(intent['tag'] not in classes):\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "print(\"classes:\", classes)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2141539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words]\n",
    "\n",
    "pickle.dump(words,open('words.pkl', 'wb'))\n",
    "pickle.dump(classes,open('classes.pkl', 'wb'))\n",
    "\n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d223bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "training = []\n",
    "\n",
    "#output array\n",
    "out = [0]*len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    word_patterns = doc[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        if word in word_patterns:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    out_row = list(out)\n",
    "    out_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, out_row])\n",
    "\n",
    "    \n",
    "random.shuffle(training)\n",
    "#training =  torch.FloatTensor(training)\n",
    "\n",
    "training = np.array(training, dtype='object')\n",
    "# create training and testing lists. X - patterns, Y - intents\n",
    "train_x = np.array(list(training[:,0]))\n",
    "train_y = np.array(list(training[:,1]))\n",
    "\n",
    "\n",
    "#training = np.array(training, dtype='int64')\n",
    "#train_ds = torch.utils.data.DataLoader(training, batch_size = 5, shuffle=True, num_workers=2)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3bf7f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=88, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(len(training[:,0][0]), 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64,len(training[:,1][0]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = F.softmax(self.fc3(x), dim=0)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "#add sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ccfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss(reduction='mean')   \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(training, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        labels = torch.FloatTensor(labels).to(device)\n",
    "        inputs = torch.FloatTensor(inputs).to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "               \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    outputs = (outputs>0.5).float()\n",
    "    correct = (outputs == labels).float().sum()\n",
    "    print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1,epochs, loss.data, correct/outputs.shape[0]))\n",
    "    \n",
    "        \n",
    "print(\"Fin Train\")\n",
    "#No eval\n",
    "#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0e3567cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model.pth');\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7399f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 'model.pth'\n",
    "model = torch.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Type Exit to Exit Debug to Debug ---- \n",
      "User> Is anyone there?\n",
      "Bot>  Hi there, how can I help?\n",
      "User> What items do you have here\n",
      "Bot>  We sell coffee and tea\n",
      "User> Do you use credit cards?\n",
      "Bot>  We accept VISA, Mastercard and Paypal\n",
      "User> How long is a delivery\n",
      "Bot>  Delivery takes 2-4 days\n",
      "User> Tell me something\n",
      "Bot>  Why did the hipster burn his mouth? He drank the coffee before it was cool.\n",
      "User> Well bye have a good day\n",
      "Bot>  Have a nice day\n",
      "User> Debug\n",
      "[[{'intent': 'greeting', 'probability': 'tensor(1.0000)'}], [{'intent': 'items', 'probability': 'tensor(1.)'}], [{'intent': 'payments', 'probability': 'tensor(1.0000)'}], [{'intent': 'delivery', 'probability': 'tensor(0.9994)'}], [{'intent': 'funny', 'probability': 'tensor(0.9992)'}], [{'intent': 'goodbye', 'probability': 'tensor(0.7670)'}]]\n",
      "User> Exit\n",
      "Bot>  Hello, thanks for visiting\n",
      "User> \n",
      "Bot>  Hi there, how can I help?\n",
      "User> Exit\n",
      "Bot>  Hey :-)\n"
     ]
    }
   ],
   "source": [
    "intents = json.loads(open('context_responses.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def clean_(sentence):\n",
    "    wrds= sentence.split(' ');\n",
    "    swrds = [lemmatizer.lemmatize(wrd.lower()) for wrd in wrds]\n",
    "    return swrds\n",
    "\n",
    "\n",
    "def conv(sentence, words, show_details=True):\n",
    "    sentence_words = clean_(sentence)\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % word)\n",
    "    return(np.array(bag))\n",
    "\n",
    "\n",
    "def getResponse(tag):\n",
    "    for i in range(len(intents['intents'])):\n",
    "        if(intents['intents'][i]['tag'] == tag):\n",
    "            responses = intents['intents'][i]['responses']\n",
    "            response = random.choice(responses)\n",
    "            return response\n",
    "\n",
    "def predict_class(sentence):\n",
    "    p = conv(sentence, words,show_details=False)\n",
    "\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    model.eval()\n",
    "    model.train()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        res = model(torch.FloatTensor(p))\n",
    "    \n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "print(\"---Type Exit to Exit Debug to Debug ---- \")\n",
    "ins = \"Filler\"\n",
    "stats_chart = []\n",
    "while ins != \"Exit\":\n",
    "    ins = input(\"User> \")\n",
    "    if ins == \"Debug\":\n",
    "        if(len(stats_chart) == 0):\n",
    "            print(\"Nothing To Show\")\n",
    "            continue\n",
    "        print(stats_chart)\n",
    "        ins  = \"Filler\"\n",
    "        continue\n",
    "      \n",
    "    ins = normalize(ins)\n",
    "    stats = predict_class(ins)\n",
    "    stats_chart.append(stats)\n",
    "    print(\"Bot> \",getResponse(stats[0]['intent']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f18437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
